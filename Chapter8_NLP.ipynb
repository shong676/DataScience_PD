{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter8_NLP.ipynb","provenance":[],"authorship_tag":"ABX9TyMf8BdY7KgV2JPcrkMMJZes"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CHAPTER 8 자연어 처리 시작하기"],"metadata":{"id":"QoZcvlwgAvcw"}},{"cell_type":"markdown","source":["## 8-1 한글 자연어 처리 기초 - KoNLPy 및 필요 모듈의 설치"],"metadata":{"id":"0jJiuu88A2o0"}},{"cell_type":"markdown","source":["KoNLPy는 Lucy Park이라는 분이 개발하였습니다. 검색해 보면, 다양한 튜토리얼과 많은 배울 것들이 있기에 꼭 블로그를 방문하여 기초를 다지는 것을 추천한다고 합니다. 특히 Github ID를 검색해보면 좋은 코드들을 많이 볼 수 있다고 합니다. 아래는 제 26회 한글 및 한국어 정보처리 학술대회 논문집(2014년)에 Lucy Park 님이 KoNLPy를 개발한 결과를 발표한 논문의 요약입니다.\n","\n","> 파이썬은 간결한 아름다움을 추구하는 동시에 강력한 스트링 연산이 가능한 언어다. KoNLPy는 그러한 특장점을 살려, 파이썬으로 한국어 정보처리를 할 수 있게 하는 패키지 이다. 꼬꼬마, 한나눔, MeCab-ko 등 국내외에서 개발된 여러 형태소 분석기를 포함하고, 자연어처리에 필요한 각종 사전, 말뭉치, 도구 및 다양한 튜토리얼을 포함하여 손쉽게 한국어 분석을 할 수 있도록 만들었다."],"metadata":{"id":"fhMmJJXTDK1Y"}},{"cell_type":"markdown","source":["파이썬에서 KoNLPy를 사용하기 위해서는 `pip install konlpy`명령으로 konlpy를 설차합니다. 그리고 Java SDK다운로드 페이지에서 JDK를 다운로드 합니다."],"metadata":{"id":"IXwS7qy-D_Gf"}},{"cell_type":"markdown","source":["JDK 설치가 끝나면 JAVA_HOME설정도 해 주어야 한다고 합니다."],"metadata":{"id":"OYE1OXQqEjA4"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rEHyBM11EsiS","executionInfo":{"status":"ok","timestamp":1650326335803,"user_tz":-540,"elapsed":6477,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"14e16a7f-56e3-406c-ca5a-f12d701bcc9e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 5.8 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Collecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 46.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GoCiCSL9E1dv","executionInfo":{"status":"ok","timestamp":1650327004357,"user_tz":-540,"elapsed":172761,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"c4d585d5-9dd5-4146-9fa9-aad8fcb2ddd5"},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> d\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> l\n","Packages:\n","  [ ] abc................. Australian Broadcasting Commission 2006\n","  [ ] alpino.............. Alpino Dutch Treebank\n","  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n","  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n","  [ ] basque_grammars..... Grammars for Basque\n","  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n","                           Extraction Systems in Biology)\n","  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n","  [ ] book_grammars....... Grammars from NLTK Book\n","  [ ] brown............... Brown Corpus\n","  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n","  [ ] cess_cat............ CESS-CAT Treebank\n","  [ ] cess_esp............ CESS-ESP Treebank\n","  [ ] chat80.............. Chat-80 Data Files\n","  [ ] city_database....... City Database\n","  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n","  [ ] comparative_sentences Comparative Sentence Dataset\n","  [ ] comtrans............ ComTrans Corpus Sample\n","  [ ] conll2000........... CONLL 2000 Chunking Corpus\n","  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n","Hit Enter to continue: stop\n","  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n","                           and Basque Subset)\n","  [ ] crubadan............ Crubadan Corpus\n","  [ ] dependency_treebank. Dependency Parsed Treebank\n","  [ ] dolch............... Dolch Word List\n","  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n","                           Corpus\n","  [ ] extended_omw........ Extended Open Multilingual WordNet\n","  [ ] floresta............ Portuguese Treebank\n","  [ ] framenet_v15........ FrameNet 1.5\n","  [ ] framenet_v17........ FrameNet 1.7\n","  [ ] gazetteers.......... Gazeteer Lists\n","  [ ] genesis............. Genesis Corpus\n","  [ ] gutenberg........... Project Gutenberg Selections\n","  [ ] ieer................ NIST IE-ER DATA SAMPLE\n","  [ ] inaugural........... C-Span Inaugural Address Corpus\n","  [ ] indian.............. Indian Language POS-Tagged Corpus\n","  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n","                           ChaSen format)\n","  [ ] kimmo............... PC-KIMMO Data Files\n","  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n","Hit Enter to continue: \n","  [ ] large_grammars...... Large context-free and feature-based grammars\n","                           for parser comparison\n","  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n","  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n","                           part-of-speech tags\n","  [ ] machado............. Machado de Assis -- Obra Completa\n","  [ ] masc_tagged......... MASC Tagged Corpus\n","  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n","  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n","  [ ] moses_sample........ Moses Sample Models\n","  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n","  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n","  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n","                           2015) subset of the Paraphrase Database.\n","  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n","  [ ] nombank.1.0......... NomBank Corpus 1.0\n","  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n","  [ ] nps_chat............ NPS Chat\n","  [ ] omw-1.4............. Open Multilingual Wordnet\n","  [ ] omw................. Open Multilingual Wordnet\n","  [ ] opinion_lexicon..... Opinion Lexicon\n","Hit Enter to continue: \n","  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n","  [ ] paradigms........... Paradigm Corpus\n","  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n","                           Evaluation Shared Task\n","  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n","                           character properties in Perl\n","  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n","  [ ] pl196x.............. Polish language of the XX century sixties\n","  [ ] porter_test......... Porter Stemmer Test Files\n","  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n","  [ ] problem_reports..... Problem Report Corpus\n","  [ ] product_reviews_1... Product Reviews (5 Products)\n","  [ ] product_reviews_2... Product Reviews (9 Products)\n","  [ ] propbank............ Proposition Bank Corpus 1.0\n","  [ ] pros_cons........... Pros and Cons\n","  [ ] ptb................. Penn Treebank\n","  [ ] punkt............... Punkt Tokenizer Models\n","  [ ] qc.................. Experimental Data for Question Classification\n","  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n","                           version\n","  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n","                           Portuguesa)\n","Hit Enter to continue: \n","  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n","  [ ] sample_grammars..... Sample Grammars\n","  [ ] semcor.............. SemCor 3.0\n","  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n","  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n","  [ ] sentiwordnet........ SentiWordNet\n","  [ ] shakespeare......... Shakespeare XML Corpus Sample\n","  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n","  [ ] smultron............ SMULTRON Corpus Sample\n","  [ ] snowball_data....... Snowball Data\n","  [ ] spanish_grammars.... Grammars for Spanish\n","  [ ] state_union......... C-Span State of the Union Address Corpus\n","  [ ] stopwords........... Stopwords Corpus\n","  [ ] subjectivity........ Subjectivity Dataset v1.0\n","  [ ] swadesh............. Swadesh Wordlists\n","  [ ] switchboard......... Switchboard Corpus Sample\n","  [ ] tagsets............. Help on Tagsets\n","  [ ] timit............... TIMIT Corpus Sample\n","  [ ] toolbox............. Toolbox Sample Files\n","  [ ] treebank............ Penn Treebank Sample\n","  [ ] twitter_samples..... Twitter Samples\n","Hit Enter to continue: stopwords\n","  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n","                           (Unicode Version)\n","  [ ] udhr................ Universal Declaration of Human Rights Corpus\n","  [ ] unicode_samples..... Unicode Samples\n","  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n","  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n","  [ ] vader_lexicon....... VADER Sentiment Lexicon\n","  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n","  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n","  [ ] webtext............. Web Text Corpus\n","  [ ] wmt15_eval.......... Evaluation data from WMT15\n","  [ ] word2vec_sample..... Word2Vec Sample\n","  [ ] wordnet2021......... Open English Wordnet 2021\n","  [ ] wordnet31........... Wordnet 3.1\n","  [ ] wordnet............. WordNet\n","  [ ] wordnet_ic.......... WordNet-InfoContent\n","  [ ] words............... Word Lists\n","  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n","                           English Prose\n","\n","Collections:\n","  [ ] all-corpora......... All the corpora\n","Hit Enter to continue: \n","  [ ] all-nltk............ All packages available on nltk_data gh-pages\n","                           branch\n","  [ ] all................. All packages\n","  [ ] book................ Everything used in the NLTK Book\n","  [ ] popular............. Popular packages\n","  [ ] tests............... Packages for running tests\n","  [ ] third-party......... Third-party data packages\n","\n","([*] marks installed packages)\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> stopwords\n","    Downloading package stopwords to /root/nltk_data...\n","      Unzipping corpora/stopwords.zip.\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> punkt\n","Command 'punkt' unrecognized\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["pip install wordcloud"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0-zV1VZJQBP","executionInfo":{"status":"ok","timestamp":1650327020910,"user_tz":-540,"elapsed":2974,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"444ce0a9-53e4-445a-a6dc-b8f71318400a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.21.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n"]}]},{"cell_type":"code","source":["pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GeESTY0oJ9XA","executionInfo":{"status":"ok","timestamp":1650327046760,"user_tz":-540,"elapsed":3668,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"fa9eb940-0de9-4835-f8d2-2c295284e4f5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"]}]},{"cell_type":"markdown","source":["## 8-2 한글 자연어 처리 기초"],"metadata":{"id":"KCAStQmVKAXN"}},{"cell_type":"markdown","source":["KoNLPy는 꼬꼬마, 한나눔 등의 엔진을 사용할 수 있게 합니다.\n","\n","먼저 꼬꼬마 모듈을 사용할 수 있게 합니다."],"metadata":{"id":"9i8dWr9kKM36"}},{"cell_type":"code","source":["from konlpy.tag import Kkma\n","kkma = Kkma()"],"metadata":{"id":"b_GBYqcgKHGl","executionInfo":{"status":"ok","timestamp":1650327076445,"user_tz":-540,"elapsed":1774,"user":{"displayName":"홍성진","userId":"08892775570803704959"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["그리고 문장(sentences)분석을 합니다. \\~\\~시작합니다 재미있어요 \\~\\~ 에서 마침표(.)가 없어도 두 개의 문장으로 구분합니다."],"metadata":{"id":"9BsCDpgSKLVN"}},{"cell_type":"code","source":["kkma.sentences('한국어 분석을 시작합니다 재미있어요 ~~')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dbe4mvHIKcH4","executionInfo":{"status":"ok","timestamp":1650327231878,"user_tz":-540,"elapsed":432,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"8e88f9fe-1fa6-4261-e1d0-77e1c23b2d62"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['한국어 분석을 시작합니다', '재미있어요 ~~']"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["명사(nouns) 분석도 수행합니다."],"metadata":{"id":"y8UvfuCXKfw0"}},{"cell_type":"code","source":["kkma.pos('한국어 분석을 시작합니다 재미있어요~~')"],"metadata":{"id":"2U7cmI1HK2QO","executionInfo":{"status":"ok","timestamp":1650327271588,"user_tz":-540,"elapsed":2,"user":{"displayName":"홍성진","userId":"08892775570803704959"}},"outputId":"11ff582a-59b5-40ff-99ba-a14d2260170f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('한국어', 'NNG'),\n"," ('분석', 'NNG'),\n"," ('을', 'JKO'),\n"," ('시작하', 'VV'),\n"," ('ㅂ니다', 'EFN'),\n"," ('재미있', 'VA'),\n"," ('어요', 'EFN'),\n"," ('~~', 'SW')]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[""],"metadata":{"id":"mjM9Cjl3K7Ts"},"execution_count":null,"outputs":[]}]}